---
title: "Motivation for the bivariate normal distribution: an example (of what it is not)"
author: "Fernando Pazos Ruiz"
output: 
 bookdown::html_document2: default
---
This article does not intend to actually explain what the bivariate normal distribution is but rather expose an example and, through it, reflect on the absence of properties that would be seen if we were dealing with a bivariate normal distribution. We will, nonetheless, mention the concept a few times. Whenever you see the concept is mentioned try to think how would the generalization of a normal density look like for two variables: it would be a three-dimensional bell sitting on top of a two-dimensional plane.

&nbsp;

Let $I$ be a random variable that follows a Bernoulli distribution with parameter $p=0.5$. The set of values that this variable can take ---i.e. its range--- is $\{0, 1\}$, and to each of those values is assigned a probability of $0.5$.

Consider $J=2I-1$. Random variable $J$ thus defined is said to follow the *Rademacher distribution*.

$$P_J(j) = \frac{1}{2} \hspace{2mm} \text{for} \hspace{2mm} j \in \{-1, 1\}$$

&nbsp;

Let $J$ be a random variable that follows the Rademacher distribution. Let $X$ be a standardized normally distributed variable **independent from $J$.**

Now, lets define $Y=JX$.

We'll be answering ---both practically and theoretically--- the following questions: 

1. what is the pdf of $Y$?   
2. are $X$ and $Y$ correlated?   
3. are $X$ and $Y$ independent one from each other?
4. what is the joint density of $X$ and $Y$?

&nbsp;

&nbsp;

# The pdf of $Y${#pdf}

For a practical answer, we will generate a random sample of size $1000$ of each of the variables ---$X$ and $Y$---, and we will do a visual inspection of their distributions.

```{r Histograms, out.width = '50%', fig.cap = 'Histograms of the samples', fig.show = 'hold', fig.asp = 0.9, fig.align = 'center', echo = TRUE}
# Previous installation of package stats is required

sample_j = stats::rbinom(1000, 1, 1/2)*2 - 1
sample_x = rnorm(1000, 0, 1)
sample_y = sample_j*sample_x

hist(sample_x, freq = FALSE, xlab = "X", main = "Histogram of X")
hist(sample_y, freq = FALSE, xlab = "Y", main = "Histogram of Y")
```
&nbsp;

**Both densities seem to be very much alike** ---bell-shaped, centered around the value of zero and exhibiting the same distance between the lowest and the largest value of their respective range---. This same behaviour is observed regardless of the seed ---and that is why I didn't put any---.

&nbsp;
&nbsp;

Now, let us make the formal derivation of the pdf of $Y$.

The following is an application of the total probability theorem:  
\begin{equation}
f_Y(y) = P_J(-1)\cdot f_{Y|J}(y|-1) + P_J(1)\cdot f_{Y|J}(y|1) (\#eq:totalprobability)
\end{equation}


&nbsp;

We are going to need to calculate the conditional density of $Y$.

&nbsp;

Given $J=j$, we have $Y=jX$. Therefore, 

\begin{equation}
P(Y \leq t|J=j) = P(jX \leq t|J=j) 
\end{equation}

&nbsp;

&nbsp;

If we plug $\hspace{1mm}j=-1 \hspace{1mm}$ into the equation above we get

\begin{equation}
P(Y \leq t|J=-1) = P(X \geq -t|J=-1) (\#eq:jminus1)
\end{equation}

&nbsp;

&nbsp;

Since $X$ and $J$ are independent ---**that is one of our assumptions**--- we can claim that $P(X \geq -t|J=-1) = P(X \geq-t)$.

By replacing this on the right hand-side of equation \@ref(eq:jminus1) we obtain

\begin{align}
P(Y \leq t|J=-1) &= P(X \geq-t) \\
&= 1 - P(X \leq -t ) \\ \\
&= 1 - \int_{-\infty}^{-t}f_X(x)dx \\
\end{align}

&nbsp;

Next, we take the derivative of both sides of this equation with respect to $t$.

\begin{align}
\hspace{19.5mm} f_{Y|J}(t|-1) &= -\frac{d}{dt} \Big(\int_{-\infty}^{-t}f_X(x)dx\Big) \\ \\
 &= -f_X(-t)\cdot \frac{d}{dt}(-t) \\ \\
 &= f_X(-t) \newline
 &= f_X(t)
\end{align}

Regarding the last step in our derivation, it is legitimate due to the fact that the standard normal distribution is symmetric with respect to the value of $0$. 

&nbsp;

**By an analogous approach it is not hard to show that $\hspace{1mm} f_{Y|J}(t|1) = f_X(t)$.**

&nbsp;

Now, $t$ serves as nothing more than a placeholder. We are allowed to denote the argument of function $f_{Y|J}$ however we want. Actually, it feels more natural to denote it $y$.


Replacing the conditional densities ---and the respective probabilities of the different values of $j$--- on equation \@ref(eq:totalprobability) yields: $$f_Y(y) = f_X(y)$$

This result leaves no doubt: the reason why samples of $X$ have histograms similar to those of the samples of $Y$ is because **$X$ and $Y$ have the same distribution** ---i.e. they are _identically distributed_---.

&nbsp;

&nbsp;


# Correlation between $X$ and $Y$ 

Before tackling this problem mathematically, why don't we compute Pearson's correlation coefficient between the two samples?

```{r}
cor(sample_x, sample_y)
```
The absolute value of the correlation coefficient that you are going to get is probably not greater than $0.2$^[No fancy calculation has led me to this conclusion, I just checked with different seeds.]. That being so, we should expect the *theoretical* correlation to be low.

&nbsp;

The covariance between $X$ and $Y$ is 

\begin{align}
Cov(X, Y) &= E[(X - E[X])\cdot(Y - E[Y])] \\
&= E[X\cdot Y] \\
&= E[X\cdot JX] \\
&= E[JX^2] \\
&= E[J] \cdot E[X^2] \\
\end{align}

The mathematical expectation of the product of random variables is not necessarily equal to the product of the expectations of each variable **unless** these variables are independent. And indeed, since $X$ and $J$ are independent, so are $X^2$ and $J$.

&nbsp;

The mean of the Rademacher distribution is $E[J] = 0$. Therefore, $$Cov(X, Y) = 0$$ 

**The variables $X$ and $Y$ are uncorrelated.**

&nbsp;

&nbsp;

# Are $X$ and $Y$ independent?{#ind}

Actually, this question stems from another ---more general--- question: does zero correlation imply independence? **Unless some other conditions are met** it does not.

Two random variables are said to be independent if our knowledge about one of them doesn't allow us to predict more precisely what the realization of the other will be.

The random variable $Y$ has two sources of randomness. Its absolute value is random because $X$ is random. Its sign is random because $J$ is random.

Suppose that we know that the realization of the random variable $X$ is $-1.1$. In that case, $Y = -1.1 \cdot J$. Suddenly, the only source of the randomness of $Y$ is $J$. **Knowing what the realization of $X$ is removes uncertainty on what the magnitude of $Y$ is. Now the only uncertainty that remains is whether $Y$ will be a positive or a negative magnitude.** It can be either with probability $0.5$.

Let's test this with our sample, shall we?

```{r}
y_close_minus_1.1 = sample_y[sample_x >= -1.6 & sample_x <= -0.6]

length(y_close_minus_1.1[y_close_minus_1.1 < 0])/length(y_close_minus_1.1)
```
We have selected all observations of the sample of $Y$ that, **before being multiplied by our sample of $J$**, used to be close to $-1.1$ (between $-1.6$ and $-0.6$). Then, out of those observations, we calculated the fraction of negative values **after being multiplied by our sample of $J$.** The fraction is around half, as expected.

That settles it: **$X$ and $Y$ are not independent**.

&nbsp;

&nbsp;

# The joint pdf of $X$ and $Y$
The left panel of Figure \@ref(fig:Range) depicts the set of all possible values that the random vector $(X, JX)$ may take.

```{r Range, out.width = '50%', fig.cap = 'Comparison with the bivariate normal', fig.show = 'hold', fig.asp = 0.9, fig.align = 'center', echo = FALSE}
plot_x = -100:100
y_1 = -100:100
y_2 = 100:-100
plot(plot_x, y_1, type = 'l', xlab = "Range of X", ylab = "Range of Y", asp = 1, xaxt = 'n', yaxt = 'n', main = "Joint range of the variables that concern us")
lines(plot_x, y_2)

x_axis = rnorm(100000, 0, 1)
y_axis = rnorm(100000, 0, 1)

plot(x_axis, y_axis, xlab = "", ylab= "", main = "Big sample of a bivariate normal vector with zero correlation")
```

&nbsp;

We have two straight lines intersecting at $(0, 0)$. One of them has slope $1$, the other has slope $-1$.

With probability $0.5$ the condition $J=1$ is satisfied and, therefore, the relationship between $X$ and $Y$ is represented by the upward sloping line. Complementarily, the condition $J=-1$ may be satisfied instead, and in such case it is the downward sloping line that represents the relationship between $X$ and $Y$.

It is fair to say, then, that the total probability is cut into two halves and *each half is concentrated on each one of the lines*. **This is sufficient for us to claim that $X$ and $Y$ are not jointly continuous.**

**In order for two random variables to be jointly continuous it is required** not only that each variable is continuous ---i.e. not discrete---, but also **that the joint range of the variables is a surface^[In order for $n$ random variables to be jointly continuous it would be necessary (and sufficient) for their joint range to be a $n$-dimensional set. For example, we would require for the joint range of three jointly continuous random variables to be a set endowed with volume. Hence, it would make sense to claim that there exists a function that assigns a quantity of probability per unit of volume.]** ---i.e. that it is a set with length and breadth---**.** 

&nbsp;

For comparison, on the right panel of Figure \@ref(fig:Range) we have depicted a joint sample of size $100'000$ of two random variables that ---just like the variables that we are contemplating--- are normally distributed, uncorrelated and each one with mean $0$ and variance $1$, but ---**unlike our variables**--- **they are independent.**

The silhouette of this sample insinuates the shape of an ellipse^[There is a logical reason for that. Remember what we had said at the beginning of the article: the density of the bivariate normal distribution is a three-dimensional bell. How do the level curves of a bell look like? They are concentric ellipses.]. **Note that the ellipse does encompass a surface.** In the case of these variables, **the probability is not concentrated on one-dimensional sets but, rather, it is spread across a surface.** Then, these variables do have a joint density, which is that of the bivariate normal distribution.  

&nbsp;

&nbsp;

# Closing remarks{-}
On section \@ref(ind) we stated that under some conditions uncorrelatedness does imply independence. Now, in light of what we just discussed on the previous section, we are able to deduce what those conditions are.

We have seen that in the case of a random vector consisting of two **uncorrelated and independent** standard normal variables it is legitimate to talk about density ---i.e. it is legitimate to talk about a quantity of probability *per unit of area*---. The specifications of how dense the probability is in each of the points belonging to the surface^[This is a somehow weird abstraction in mathematics: since a surface ---an entity capable of having a density--- is made up of points then the points that comprise it have their own density even though points don't have dimension. Generalizing, **what grants to a point the right to have a density is _1._ membership to a set that has an amount of dimensions equal to the amount of r.v.s that are the components of the random vector and _2._ for the aforementioned set to be the set of all possible values that the random vector can take ---i.e. the set that we also call the joint range---.**]--- is given according to the probability density function of the **bivariate normal distribution^[Since the joint range of the variables is the whole set $\mathbb{R}^2$ *every* point on that set has a density associated to it. However, the density assigned to points far off the center of the concentric ellipses ---whose coordinates are the means of the variables--- is very negligible. That is the reason why, in case you were wondering, on the right panel of Figure \@ref(fig:Range) all the observations seem to be bounded ---you will almost certainly not see an observation exhibiting an absolute value higher than 5 on any of its coordinates---. **That sort of boundedness is a characteristic feature of the normal distribution** ---be it univariate, bivariate or multivariate---.].**  

In contrast, two **uncorrelated and yet dependent** standard normal variables **do not** ---when considered jointly--- **follow the bivariate normal distribution** because they actually do not even have a joint density.

Then, if it was stated that a pair of normal variables is known to follow ---when considered jointly--- the bivariate normal distribution and it was also stated that these variables are uncorrelated there is no chance for the variables other than to be necessarily independent. 

**Uncorrelatedness between two normal variables implies independence if and only if the variables follow the bivariate normal distribution.**

There is something remarkable about the fact that on this last claim we dropped any assumption about the variables involved being standard. Well, that omission didn't happen out of carelessness. It is because **this conclusion applies to any pair of normal random variables regardless of the parameters of their marginal distributions.** 

It was just for the sake of this particular example that we assumed $X$ to be standard and, as a consequence of it, $Y$ becomes a standard *normal* as well. What effect does assuming other values for the mean and variance of $X$ have on $Y$? I encourage you to tamper with the code chunk from section \@ref(pdf) and see for yourself.
